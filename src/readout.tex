\section{Read-out and data processing (20p.)}
%\subsection{O2 Overview: reconstruction workflow (PDP) (Andreas; 5 p.)}
\subsection{${\rm O}^2$ and Data Processing Overview}
The continuous rad-out of the large majority of the detectors, is one of the most important changes with respect to Run1/2.
At the projected peak Pb--Pb interaction rate of 50~kHz the data throughput from the 
detectors is expected to be 3.75~TB/s, approximately a factor of 50 higher than during Run 2.
In order to minimize the costs and compute time of the offline system for data 
processing and storage, the ALICE Computing Model for Runs 3 and 4 is designed 
for a maximum reduction of the data volume synchronous with the data taking and without rejecting events. 
This task is achieved in two processing steps on the ALICE online/offline facility (${\rm O}^2$) located at Point2. It consists of two types of compute nodes, the First Level Processors (FLP) located in the 
experiment access shaft (CR1) and the Event Processing Nodes (EPN) at CR0. In addition to the compute 
nodes it provides networking, data storage as well as interfaces with the Grid and the permanent data 
store at the Tier
0. For more technical details on these clusters, see sections \ref{sec:flp} and \ref{sec: epn}.

Data produced by the detectors are transferred to the common read-out units (CRU) in a continuous or
physics triggered read-out mode. The triggered data will be tagged with the LHC clock information while 
the continuous streams of data samples are split into so called heartbeat frames (HBF) tagged with the 
corresponding HB ID. Nominally, the time between two HBs is $89.4 \; \mu s$ (one LHC orbit period).
The data are compressed and multiplexed
in the CRUs and transferred to the memory of the FLPs.
The 272 FLPs perform a 
first level of data compression to 635~GB/s by zero suppression and clustering as well as calibration 
tasks based on local information from the part of the detector they serve, for example base-line 
correction.
Moreover, HBFs are accumulated into sub time frames (STF) of about 11-22~ms length (128-256 LHC orbits) 
containing $\approx 550-1100$ collisions. A dedicated FLP collects and processes data from the Detector 
Control System (DCS).
The STFs are then dispatched to the Event Processing Nodes (EPN). 

STFs related to the same time period and from all FLPs are received by the same EPN and aggregated into 
a complete time frame (TF).
The EPN farm consists of 250 servers hosting each 8 GPUs and 
64 CPU cores. The capacity has been dimensioned such that it can achieve
a first pass online synchronous reconstruction, extraction of calibration 
objects for subsequent asynchronous reconstruction passes and data reduction. 
Data is aggregated into so called compressed time frames (CTF) replacing the original raw data and 
written to a 
60~PB disk buffer at an output rate of 90~GB/s. Calibration data is aggregated and stored in the 
Calibration and Constants Data Base (CCDB). 

Two third of the CTFs are transferred to T0 and one third to T1 for archiving. 
After data taking and full detector calibration, at least two asynchronous 
reconstruction passes on T0 and T1 centers as well as on the EPN farm are 
performed. The output of these reconstruction passes is stored as Analysis 
Object Data (AOD), the input for physics analysis. For specific physics signals, a further data size 
reduction and speed-up of the corresponding analyses is achieved by filtering out events of interest and writing out only the minimum event information needed.
The processing of pp data will follow the same chain with the addition of the selection of interesting 
events during an asynchronous reconstruction pass with reduction of the CTFs by keeping only the 
clusters associated to these events. Reconstruction passes are followed by Monte Carlo production cycles taking into account the time dependent detector conditions.

Besides the compute infrastructure a common software framework has
been developed in collaboration with GSI (FAIR) within which all 
online and offline components are developed and operate. It 
consists of three main components. 
The {\it Transport Layer} is implemented using the FairMQ message 
passing toolkit. It enables efficient parallelism by providing 
abstraction of network and inter process communication as well as 
by supporting shared memory backed message passing. The {O2 Data 
Model} is message passing aware and provides support for various 
backends such as a performance optimized simplifies 0-copy format, ROOT based serialisation and Apache Arrow for analysis and and 
integration with external tools. Finally, the {\it Data Processing
Layer} (DPL) abstracts computation as a set of data processors 
organized in a logical dataflow explaining of data is transformed.

\begin{figure*}[hbtp]
 \begin{center}
  %\includegraphics[width=.5\textwidth]{<det>/<filename>}
 \end{center}
 \caption{Synchronous Reconstruction Workflow}
 \label{fig:rwf}
 \end{figure*}
\subsubsection{Synchronous Reconstruction}
A schematic representation of the synchronous reconstruction workflow is shown in Fig. \ref{fig:rwf}.
The main objective of synchronous processing is to reduce the data rate from the TPC which is with 90\%
the dominant contributor. This is achieved by performing clustering and full track reconstruction in the
TPC. Hits not attached to physical tracks are removed from the data. Moreover, cluster space point 
coordinates are stored as relative coordinates, thus reducing the entropy and rendering entropy encoding
more efficient. In addition, detector calibration information is extracted replacing the additional 
calibration passes in front of the production reconstruction passes needed in Run1/2. TPC space charge 
distortion calibration uses the information of fully reconstructed barrel tracks including ITS, TOF and
TRD information. However, only a small fraction of all tracks are need to be reconstructed to gain 
sufficient statistics. Hence TPC reconstruction is the most compute demanding step. To be able to 
reconstruct Pb-Pb collision at a rate of 50 kHz in a cost 
effective way it is performed on Graphic Processing Units (GPU). They provide a significant speed-up 
with respect to CPUs (at least a factor of 50) without compromising the physics performance.

\subsubsection*{GPU Processing and Data Rejection}
TPC reconstruction starts with the cluster finding. It is followed by tracking comprising the track 
finding, track merging and fitting steps. These require a first order (average) space charge distortion
correction (see below). Two options for TPC data rejection are supported by the software. In the first 
option (A) clusters of identified background clusters (for example from noisy pads or charge clouds 
related to low momentum protons) and background tracks, such as very low momentum tracks, loopers and 
tracks with large inclination angle, are rejected.  For the second option (B) only clusters attached or
in the proximity of identified signal tracks are kept.
The estimated rejection fractions for options A and B are $12.5-39.1\%$ and $37-53\%$, respectively. 
While option B yields lower data size it is more risky in case of calibration problems. Optimal 
performance of option A requires identification of hits from particles with momentum below 10~GeV/$c$, 
which is still under development.

Further data size reduction is achieved by converting the cluster properties from the single-precision 
floating point format used in reconstruction to an integer format with exactly as many bits as needed 
for the intrinsic TPC resolution. For entropy reduction, coordinates of clusters that are not assigned 
to tracks are stored as differences to the previous cluster and those of clusters assigned to tracks 
are stored relative to the extrapolated track (Track Model Compression). Cluster charges are stored 
relative to the ${\rm d}E/{\rm d}x$ of the track and cluster size relative to the average size of 
clusters of the same track.

While for synchronous reconstruction TPC tracking is by far the most beneficial to be offloaded to GPUs, for the 
asynchronous reconstruction passes global reconstruction including ITS tracking will dominate. To this 
end other reconstruction components have been already ported to GPU with the final goal to run there 
all barrel tracking. The reconstruction code is written using generic C++ code and can run on different
GPU hardware. This opens the possibility to run reconstruction efficiently on heterogeneous  
computing platforms that will become available on the GRID:

\subsubsection*{\bf CPU Processing}
For ITS and the muon spectrometer system (MFT, MCH, MID) synchronous processing starts with space point reconstruction (clustering). For the barrel calorimeters EMCAL and PHOS, the cell properties (time, amplitude) are determined by fitting the raw time distributions. Clusterization is performed in order to select cells to write to the CTF, while final clustering will be performed in the asynchronous reconstruction passes. Data for time calibration and dead-channel maps are extracted. For FT0, th the reconstruction of collision time and vertex z-position is performed.

For about 1\% of the tracks (5\% of the tracks in the 20\% most peripheral
collisions) full tracking including all 
barrel detectors is performed, i.e. ITS tracking after clustering, matching of ITS tracks to TPC tracks
and finally track matching to TRD and TOF. As in Run2, residuals between 
global tracks and TPC clusters 
are used to create space charge distortion maps averaged over about 10~min 
time periods. These maps 
scaled by the instantaneous luminosity are used to correct during 
synchronous reconstruction the TPC 
cluster positions with a precisionof \cal{O}(mm) which is sufficient for correct cluster associations.
Global barrel tracks 
provide also a fast TPC drift 
time calibration and TRD calibration (gain, $t_0$, $E \times B$ and drift 
velocity). Moreover, time-of-flight measured by TOF is aligned with respect
to the LHC clock phase (LHC clock phase calibration) and the data for channel 
time-of-flight offset calibration is extracted.

In a final step clusters and remaining raw signals (FIT, ZDC, EMCAL, PHOS, HMPID) are compressed into CTFs using entropy encoding with the rANS algorithm, a variant of 
Asymmetric Numeral System coders.


\subsection{CTP (David; 3p.)}
\subsection{CRU (Tivadar, Alex; 3 p.)}
\subsection{FLP (Pierre; 3p.)}
\input{o2_flp}
\subsection{EPN (Volker; 3p.)}
\subsection{Grid computing (Andreas; 3 p.)}
